{
  "metadata": {
    "kernelspec": {
      "display_name": "Python (Pyodide)",
      "language": "python",
      "name": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "1818a346-0dee-47f1-ae2e-68c5586eff79",
      "cell_type": "markdown",
      "source": "# Previsão do Diagnóstico de Tumores Cerebrais\n### Aprendizagem Automática - Licenciatura em Engenharia Informática\n\nTrabalho realizado por:\n- André Gonçalves nº58392\n- André Zhan nº58762\n- Diogo Pina nº58049\n\nDepartamento de Informática, Universidade de Évora, 22 de Dezembro de 2025\n\n## Introdução\nEste notebook documenta o trabalho desenvolvido para o desafio Kaggle \"Diagnóstico de Tumores Cerebrais\". O conjunto de dados combina atributos demográficos dos pacientes com medidas de textura extraídas de imagens ADC (por fatia), sendo a unidade de predição o paciente (agregando múltiplas fatias por id).\n\nO objetivo deste trabalho é construir modelos, usando as técnicas aprendidas, que classifiquem tumores como maligno (1) ou benigno (0), maximizar a métrica F1 (medida oficial do desafio) e produzir submissões robustas para a leaderboard do Kaggle.",
      "metadata": {}
    },
    {
      "id": "0daf6cf3-bede-4a7f-aede-cbe9340f5f0e",
      "cell_type": "markdown",
      "source": "## Caracterização e análise do conjunto de dados\nO conjunto de dados contém atributos demográficos do paciente (idade e sexo) e 18 medidas de textura extraídas de imagens ADC (por fatia). Os dados incluem múltiplas fatias por paciente e cada linha do ficheiro do conjunto de dados corresponde a uma fatia identificada por id_fatia e ao id do paciente.\n\nA unidade de predição é o paciente. Para treinar e validar modelos agregamos as fatias por id (por ex.: média). Esta agregação reduz variabilidade intra-paciente e transforma o problema numa classificação binária por paciente (diagnóstico: 0=benigno, 1=maligno).\n\nO dataset contém colunas numéricas (features de textura) e categóricas/demográficas. Colunas com valores nulos devem ser listadas e tratadas por imputação (mediana para numéricas; moda para categóricas).",
      "metadata": {}
    },
    {
      "id": "4f3d07e4-bf29-4a8f-93bb-5903c5a84d4f",
      "cell_type": "markdown",
      "source": "## Imports usados\nEm baixo serão indicados os imports usados no código desenvolvido para a construção e submissão dos diferentes modelos testados:",
      "metadata": {}
    },
    {
      "id": "b0d24d3f-bf5c-4453-838a-2943fe06c8d2",
      "cell_type": "code",
      "source": "import argparse  # CLI args (e.g., --stratified)\nimport os        # filesystem ops (mkdirs)\nimport json      # read/write JSON\n\nimport numpy as np            # numeric ops, dtypes\nimport pandas as pd           # DataFrame I/O & manipulation\nfrom sklearn.model_selection import KFold, StratifiedKFold, GridSearchCV  # CV & grid search\nfrom sklearn.pipeline import Pipeline  # preprocess + model\nfrom sklearn.compose import ColumnTransformer  # column-wise transforms\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler  # encoding, scaling\nfrom sklearn.impute import SimpleImputer  # missing value imputation\nfrom sklearn.metrics import f1_score, make_scorer  # metric and scorer\nimport joblib  # save/load models",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "id": "1c3d8a31-f427-42aa-a21d-21eb4b6d3802",
      "cell_type": "markdown",
      "source": "Além disso, também faziamos import do(s) algoritmo(s) a usar, como por exemplo:",
      "metadata": {}
    },
    {
      "id": "e772a0ca-83b8-4f8e-9b9a-2be03394703b",
      "cell_type": "code",
      "source": "from sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\n# Estes dois algoritmos serão usados para a demonstração do código mais à frente",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 2
    },
    {
      "id": "79589e83-e3f6-4310-85cf-ad596b2cc90c",
      "cell_type": "markdown",
      "source": "## Agregação das Fatias por Id e Atributos Utilizados\nA agregação das fatias numa única linha por id é feita através da função aggregate_patient_features, cujas etapas são:\n- Remoção de identificadores e do 'diagnostico' (não são features);\n- Separar colunas numéricas das categóricas;\n- Para cada coluna numérica calcula-se mean, std, min, max, median, skew e kurt (kurtose), para resumir a variação e distribuição dos valores de intensidade do tumor;\n- Para cada coluna categórica pega-se a moda (valor mais frequente). Se não houver moda, pega o primeiro elemento (esta etapa garante que o atributo é único e consistente);\n- Adiciona 'n_slices' como contagem de fatias por paciente;\n- Agrupa por id todas as agregações;\n- Retorna um DataFrame com uma linha por paciente (id) contendo todas as features agregadas.\n\nEsta agregação é importante para converter a representação volumétrica fragmentada das fatias num vetor de características denso e estatisticamente representativo de cada paciente, consolidando assim toda a informação dispersa numa única instância de treino robusta e independente.\n\nDe seguida está a implementação desta parte:",
      "metadata": {}
    },
    {
      "id": "bab495df-05b1-490f-8224-d95712732a91",
      "cell_type": "code",
      "source": "def aggregate_patient_features(df):\n    df = df.copy()\n    # excluir identificadores e target das features\n    exclude = {\"id\", \"id_fatia\", \"diagnostico\"}\n    feat_cols = [c for c in df.columns if c not in exclude]\n    # separar numéricas e categóricas\n    numeric_cols = df[feat_cols].select_dtypes(include=[np.number]).columns.tolist()\n    categorical_cols = [c for c in feat_cols if c not in numeric_cols]\n\n    # preparar agregações para colunas numéricas\n    num_aggs = {}\n    for c in numeric_cols:\n        # kurt é função definida inline para usar kurtosis na agregação\n        def kurt(series):\n            return series.kurt()\n        num_aggs[c] = ['mean', 'std', 'min', 'max', 'median', 'skew', kurt]\n\n    # agregação categórica: escolher o modo (mais frequente)\n    cat_aggs = {c: (lambda x: x.mode().iloc[0] if not x.mode().empty else x.iloc[0]) for c in categorical_cols}\n\n    # contar fatias por paciente\n    df['n_slices'] = 1\n\n    # agrupar por 'id' e aplicar agregações numéricas + soma de n_slices\n    grouped_num = df.groupby('id').agg({**num_aggs, **{'n_slices': 'sum'}})\n\n    # renomear colunas agregadas (agg produz tuplas para (col, agg))\n    new_cols = []\n    for col in grouped_num.columns:\n        if isinstance(col, tuple):\n            agg_name = col[1].__name__ if callable(col[1]) else col[1]\n            new_cols.append(f\"{col[0]}_{agg_name}\")\n        else:\n            new_cols.append(col)\n    grouped_num.columns = new_cols\n    grouped_num = grouped_num.reset_index()\n\n    # agregar categóricas separadamente (usando modo) e juntar\n    if len(cat_aggs) > 0:\n        grouped_cat = df.groupby('id').agg(cat_aggs).reset_index()\n        grouped = grouped_num.merge(grouped_cat, on='id')\n    else:\n        grouped = grouped_num\n\n    # se existir coluna 'diagnostico', anexar o rótulo do paciente (usa max para consolidar)\n    if 'diagnostico' in df.columns:\n        diag = df.groupby('id')['diagnostico'].max().reset_index()\n        grouped = grouped.merge(diag, on='id')\n\n    return grouped",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 3
    },
    {
      "id": "e68c0922-9753-4bea-842e-ab571de2efba",
      "cell_type": "markdown",
      "source": "## Preprocessamento do DataFrame\nIdentifica colunas numéricas e categóricas (trata sexo como categórica); para as numéricas aplica imputação pela mediana e StandardScaler (média 0, desvio 1), para as categóricas usa OneHotEncoder com drop='first' (evita dummy trap) e handle_unknown='ignore' (não falha com categorias novas); um ColumnTransformer aplica cada transformação às colunas corretas e o preprocess resultante é usado no Pipeline juntamente com o modelo.\n\nEm baixo está apresentado o código para o preprocessamento:",
      "metadata": {}
    },
    {
      "id": "b8552c54-aa80-4a41-8315-aae19685751a",
      "cell_type": "code",
      "source": "def build_preprocessor(X):\n    # identificar colunas numéricas\n    numeric_features = X.select_dtypes(include=[np.number]).columns.tolist()\n    # 'sexo' é tratada como categórica -> remover de numéricas se presente\n    if 'sexo' in numeric_features:\n        numeric_features.remove('sexo')\n    # colunas categóricas: dtype object ou a coluna 'sexo'\n    cat_features = [c for c in X.columns if X[c].dtype == object or c == 'sexo']\n    # pipeline numérica: imputar pela mediana e padronizar\n    from sklearn.pipeline import Pipeline as SKPipeline\n    num_pipeline = SKPipeline([\n        ('imputer', SimpleImputer(strategy='median')),\n        ('scaler', StandardScaler())\n    ])\n    # ColumnTransformer: aplica OneHot às categóricas e a num_pipeline às numéricas\n    # - drop='first' evita dummy trap (colinearidade entre colunas binárias)\n    # - handle_unknown='ignore' permite categorias novas em dados de teste\n    # - remainder='drop' descarta colunas não especificadas\n    preprocess = ColumnTransformer(\n        transformers=[\n            ('cat', OneHotEncoder(drop='first', handle_unknown='ignore'), cat_features),\n            ('num', num_pipeline, numeric_features),\n        ],\n        remainder='drop'\n    )\n    return preprocess",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 4
    },
    {
      "id": "4ea8ead3-e244-4e44-a81e-818239b8f6b4",
      "cell_type": "markdown",
      "source": "## Construção do Modelo\nNa função main é onde fazemos a construção do modelo propriamente dita, recorrendo às funções apresentadas anteriormente. Esta função aceita dois argumentos relacionados com a validação cruzada, nomeadamente o nº de folds e se a validação cruzada é ou não estratificada.\n\nAs etapas da função main são:\n- Carregar os dados do conjunto de treino;\n- Fazer a agregação das fatias;\n- Preparar o pré-processador, o algoritmo escolhido e o Pipeline;\n- Escolher o esquema de validação cruzada;\n- Configurar o GridSearchCV e executá-lo, para encontrar os melhores hiperparâmetros;\n- Por fim, salvamos o modelo e avaliamos o seu desempenho no conjunto de treino completo.\n\nEm baixo está apresentado o código da função main descrita:",
      "metadata": {}
    },
    {
      "id": "4cfd08f0-11c9-4e9c-a942-cef97e98da8c",
      "cell_type": "code",
      "source": "# -- Para exemplificar usaremos o código em que foi utilizado o Algoritmo LogisticRegression para construir o modelo --\n\ndef main(n_splits=5, stratified=False):\n    # garantir diretórios de saída\n    os.makedirs('models', exist_ok=True)\n    os.makedirs('outputs', exist_ok=True)\n\n    # carregar dados brutos e agregar por paciente\n    print('Loading treino.csv...')\n    df = pd.read_csv('data/treino.csv')\n    df_agg = aggregate_patient_features(df)\n    # X = features agregadas; y = rótulo por paciente\n    X = df_agg.drop(columns=['id', 'diagnostico'])\n    y = df_agg['diagnostico']\n    print(f'Dataset aggregated: {X.shape[0]} patients, {X.shape[1]} features')\n\n    # construir pré-processador e pipeline (preprocessamento + modelo)\n    preprocess = build_preprocessor(X)\n    lr = LogisticRegression(random_state=42, max_iter=1000)\n    pipeline = Pipeline([('preprocess', preprocess), ('model', lr)])\n\n    # escolher esquema de cross-validation (estratificado opcional)\n    if stratified:\n        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n        print('Using StratifiedKFold CV')\n    else:\n        cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n        print(f'Using KFold CV with {n_splits} splits')\n    # usar F1-weighted como métrica otimizada\n    scorer = make_scorer(f1_score, average='weighted')\n\n    # grade de hiperparâmetros para GridSearchCV\n    param_grid = {\n        'model__C': [0.001, 0.01, 0.1, 1, 10, 100],\n        'model__solver': ['lbfgs', 'liblinear', 'saga'],\n        'model__penalty': ['l2', 'l1'],\n        'model__class_weight': [None, 'balanced']\n    }\n\n    # executar GridSearchCV para encontrar melhores hiperparâmetros (refit no melhor)\n    print('Running GridSearchCV with KFold cross-validation optimized for F1-weighted...')\n    gs = GridSearchCV(pipeline, param_grid=param_grid, scoring=scorer, cv=cv, n_jobs=-1, verbose=2, refit=True)\n    gs.fit(X, y)\n    print('\\n=== GridSearchCV Results ===')\n    print('Best params:', gs.best_params_)\n    print(f'Best F1-weighted score (CV): {gs.best_score_:.4f}')\n\n    # salvar parâmetros encontrados\n    with open('models/best_params_lr.json', 'w') as f:\n        json.dump(gs.best_params_, f, indent=2)\n\n    # salvar o estimador final (pipeline com preprocess + modelo)\n    final_est = gs.best_estimator_\n    joblib.dump(final_est, 'models/model_lr.pkl')\n    print('Saved final model to models/model_lr.pkl')\n\n    # avaliar desempenho no conjunto de treino completo (apenas informativo)\n    try:\n        train_preds = final_est.predict(X)\n        train_f1_weighted = f1_score(y, train_preds, average='weighted')\n        train_f1_macro = f1_score(y, train_preds, average='macro')\n        train_f1_micro = f1_score(y, train_preds, average='micro')\n        print('\\n=== Training Set Performance ===')\n        print(f'Train F1-weighted (on full training set): {train_f1_weighted:.4f}')\n        print(f'Train F1-macro (on full training set): {train_f1_macro:.4f}')\n        print(f'Train F1-micro (on full training set): {train_f1_micro:.4f}')\n        # salvar métricas de treino em JSON\n        with open('models/train_eval_lr.json', 'w') as f:\n            json.dump({\n                'train_f1_weighted': float(train_f1_weighted),\n                'train_f1_macro': float(train_f1_macro),\n                'train_f1_micro': float(train_f1_micro),\n                'cv_best_f1_weighted': float(gs.best_score_),\n                'cv_n_splits': n_splits\n            }, f, indent=2)\n    except Exception as e:\n        # caso algo falhe na avaliação, apenas logar o erro\n        print('Could not compute train F1:', e)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 5
    },
    {
      "id": "af56cd52-887b-4946-a0c8-aea55bdd1b8b",
      "cell_type": "markdown",
      "source": "Para construir um modelo com um algoritmo diferente apenas trocávamos o algoritmo usado e os hiperparâmetros a serem testados, o resto do código e a lógica permaneciam iguais. \n\nPara casos em que utilizámos um comité de peritos, o código seria quase igual, com adição das seguintes etapas:\n- Criação da função utilitária para aplicar bootstrap;\n- Criação de amostras bootstrap com reposição;\n- Treinar os modelos com as amostras bootstrap;\n- Por fim, ensemble por soft voting, com a média das probabilidades das várias estimativas.\n\nDe seguida está apresentada a função main com as alterações descritas:",
      "metadata": {}
    },
    {
      "id": "cdf740e5-9736-4127-a0ad-93ecd6eed983",
      "cell_type": "code",
      "source": "# -- Para exemplificar usaremos o código em que foi utilizado o Algoritmo LogisticRegression e SVM para criar o comité com soft voting --\n\n# Função utilitária: gera um bootstrap sample (com reposição)\ndef bootstrap_sample(X, y, n_samples=None, random_state=None):\n    n = n_samples or X.shape[0]\n    rng = np.random.RandomState(random_state)\n    idx = rng.choice(X.index, size=n, replace=True)\n    return X.loc[idx].reset_index(drop=True), y.loc[idx].reset_index(drop=True)\n\n# Alterações na função main...\ndef main(n_splits=5, stratified=False):\n    # garantir diretórios de saída\n    os.makedirs('models', exist_ok=True)\n    os.makedirs('outputs', exist_ok=True)\n\n    # carregar dados brutos e agregar por paciente\n    print('Loading treino.csv...')\n    df = pd.read_csv('data/treino.csv')\n    df_agg = aggregate_patient_features(df)\n    # X = features agregadas; y = rótulo por paciente\n    X = df_agg.drop(columns=['id', 'diagnostico'])\n    y = df_agg['diagnostico']\n    print(f'Dataset aggregated: {X.shape[0]} patients, {X.shape[1]} features')\n\n    # construir pré-processador\n    preprocess = build_preprocessor(X)\n\n    # escolher esquema de cross-validation (estratificado opcional)\n    if stratified:\n        cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n        print('Using StratifiedKFold CV')\n    else:\n        cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n        print(f'Using KFold CV with {n_splits} splits')\n    # usar F1-weighted como métrica otimizada\n    scorer = make_scorer(f1_score, average='weighted')\n    \n    # -- antes de treinar os modelos --\n    # Cria bootstrap samples (com reposição) para LR e SVM\n    X_lr, y_lr = bootstrap_sample(X, y, random_state=42)\n    X_svm, y_svm = bootstrap_sample(X, y, random_state=43)\n\n    # ============== LOGISTIC REGRESSION (COM BOOTSTRAP) ==============\n    print('\\n--- Training Logistic Regression (bootstrap sample) ---')\n\n    # Escolha do algoritmo e criação do pipeline\n    lr_model = LogisticRegression(random_state=42, max_iter=1000, n_jobs=-1)\n    lr_pipeline = Pipeline([('preprocess', preprocess), ('model', lr_model)])\n\n    # Grade de hiperparâmetros para GridSearchCV\n    lr_param_grid = {\n        'model__C': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0],\n        'model__solver': ['lbfgs', 'liblinear'],\n        'model__penalty': ['l2']\n    }\n\n    # Executar GridSearchCV para encontrar melhores hiperparâmetros (refit no melhor)\n    print('Running GridSearchCV for Logistic Regression (bootstrap sample)...')\n    lr_gs = GridSearchCV(lr_pipeline, param_grid=lr_param_grid, scoring=scorer, cv=cv, n_jobs=-1, verbose=1, refit=True)\n    lr_gs.fit(X_lr, y_lr)\n    print('Best LR params (bootstrap):', lr_gs.best_params_)\n    print(f'Best LR F1-weighted score (CV on bootstrap): {lr_gs.best_score_:.4f}')\n\n    # Salvar os melhores hiperparâmetros encontrados e o modelo treinado\n    with open('models/best_params_lr.json', 'w') as f:\n        json.dump({k.split('__')[1]: v for k, v in lr_gs.best_params_.items()}, f, indent=2)\n    lr_final = lr_gs.best_estimator_\n    joblib.dump(lr_final, 'models/model_lr_bootstrap.pkl')\n    print('Saved LR model (bootstrap) to models/model_lr_bootstrap.pkl')\n\n    # ============== SUPPORT VECTOR MACHINE (COM BOOTSTRAP) ==============\n    print('\\n--- Training Support Vector Machine (bootstrap sample) ---')\n\n    # Escolha do algoritmo e criação do pipeline\n    svm_model = SVC(kernel='rbf', random_state=42, probability=True)\n    svm_pipeline = Pipeline([('preprocess', preprocess), ('model', svm_model)])\n\n    # Grade de hiperparâmetros para GridSearchCV\n    svm_param_grid = {\n        'model__C': [0.1, 1.0, 10.0, 100.0],\n        'model__gamma': [0.001, 0.01]\n    }\n\n    # Executar GridSearchCV para encontrar melhores hiperparâmetros (refit no melhor)\n    print('Running GridSearchCV for SVM (bootstrap sample)...')\n    svm_gs = GridSearchCV(svm_pipeline, param_grid=svm_param_grid, scoring=scorer, cv=cv, n_jobs=-1, verbose=1, refit=True)\n    svm_gs.fit(X_svm, y_svm)\n    print('Best SVM params (bootstrap):', svm_gs.best_params_)\n    print(f'Best SVM F1-weighted score (CV on bootstrap): {svm_gs.best_score_:.4f}')\n\n    # Salvar os melhores hiperparâmetros encontrados e o modelo treinado\n    with open('models/best_params_svm.json', 'w') as f:\n        json.dump({k.split('__')[1]: v for k, v in svm_gs.best_params_.items()}, f, indent=2)\n    svm_final = svm_gs.best_estimator_\n    joblib.dump(svm_final, 'models/model_svm_bootstrap.pkl')\n    print('Saved SVM model (bootstrap) to models/model_svm_bootstrap.pkl')\n\n    # ============== ENSEMBLE (média de probabilidades dos modelos treinados por bootstrap) ==============\n    print('\\n--- Ensemble (média de probabilidades dos modelos treinados por bootstrap) ---')\n    ensemble_proba = (lr_final.predict_proba(X)[:, 1] + svm_final.predict_proba(X)[:, 1]) / 2.0\n    ensemble_preds = (ensemble_proba >= 0.5).astype(int) # threshold padrão\n    ensemble_f1 = f1_score(y, ensemble_preds, average='weighted')\n\n    # Métricas individuais sobre o conjunto completo (apenas para comparação)\n    lr_f1 = f1_score(y, lr_final.predict(X), average='weighted')\n    svm_f1 = f1_score(y, svm_final.predict(X), average='weighted')\n\n    # Resultados\n    print(f'Ensemble F1-weighted: {ensemble_f1:.4f}')\n    print(f'LR F1-weighted: {lr_f1:.4f}')\n    print(f'SVM F1-weighted: {svm_f1:.4f}')\n\n    # Salva modelos treinados\n    joblib.dump({'lr': lr_final, 'svm': svm_final}, 'models/ensemble_lr_svm_models.pkl')\n    print('Saved bootstrap-trained models to models/ensemble_lr_svm_models.pkl')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 6
    },
    {
      "id": "20d005b6-2ad7-425d-bfb0-4ba95e548765",
      "cell_type": "markdown",
      "source": "O trecho de código a seguir é usado auxiliarmente para configurar os argumentos da função main:",
      "metadata": {}
    },
    {
      "id": "2c5707d8-ce6c-4405-be61-16b09f530d68",
      "cell_type": "code",
      "source": "if __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--n_splits', type=int, default=5, help='Number of KFold splits (non-stratified)')\n    parser.add_argument('--stratified', action='store_true', help='Use StratifiedKFold (optional)')\n    args = parser.parse_args()\n    # A linha em baixo está em comentário para não executar a função main e evitar erro na célula\n    # main(n_splits=args.n_splits, stratified=args.stratified)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 7
    },
    {
      "id": "c290e160-9315-483a-a827-2fbb0e687ecd",
      "cell_type": "markdown",
      "source": "## Criação das Previsões\nAgora apresentamos o código para a criação dos ficheiros de submissão (implementado num ficheiro .py separado), que são as previsões ao conjunto de teste. Para tal são carregados os dados do conjunto de teste, é feita a agregação das fatias por paciente, como feito anteriormente para o conjunto de treino e, por fim, são feitas as previsões por paciente:",
      "metadata": {}
    },
    {
      "id": "755cc7ec-e3fd-4f1a-9ef4-a4fcf34cf5a6",
      "cell_type": "code",
      "source": "def main(model_path='models/model.pkl', test_csv='data/teste.csv', output='outputs/submission.csv'):\n    # Garante que a pasta de outputs existe (onde a submissão será escrita)\n    os.makedirs('outputs', exist_ok=True)\n\n    # Verifica se o arquivo do modelo existe e carrega-o com joblib\n    # O 'model_path' deve apontar para um pipeline ou modelo serializado\n    if not os.path.exists(model_path):\n        raise FileNotFoundError(f'Model not found: {model_path}')\n    model = joblib.load(model_path)\n\n    # Lê o CSV de teste com as fatias (uma linha por fatia)\n    df_test = pd.read_csv(test_csv)\n\n    # Função local para agregar as fatias por paciente (igual ao usado no treino)\n    # Objetivo: transformar múltiplas fatias por 'id' em uma linha por paciente com estatísticas\n    def aggregate(df):\n        df = df.copy()\n        # Colunas que não são features por fatia\n        exclude = {\"id\", \"id_fatia\", \"diagnostico\"}\n        feat_cols = [c for c in df.columns if c not in exclude]\n\n        # Identifica colunas numéricas e categóricas\n        numeric_cols = df[feat_cols].select_dtypes(include=[np.number]).columns.tolist()\n        cat_features = [c for c in feat_cols if c not in numeric_cols]\n\n        # Para cada coluna numérica calculamos várias agregações (média, desvio, min, max, mediana, skew, kurtosis)\n        num_aggs = {}\n        for c in numeric_cols:\n            # kurt é função definida inline para usar kurtosis na agregação\n            def kurt(series):\n                return series.kurt()\n            num_aggs[c] = ['mean', 'std', 'min', 'max', 'median', 'skew', kurt]\n\n        # Para colunas categóricas usamos a moda (valor mais frequente)\n        cat_aggs = {c: (lambda x: x.mode().iloc[0] if not x.mode().empty else x.iloc[0]) for c in cat_features}\n\n        # Conta número de fatias por paciente\n        df['n_slices'] = 1\n\n        # Aplica agregações numéricas por 'id'\n        grouped_num = df.groupby('id').agg({**num_aggs, **{'n_slices': 'sum'}})\n\n        # renomear colunas agregadas (agg produz tuplas para (col, agg))\n        new_cols = []\n        for col in grouped_num.columns:\n            if isinstance(col, tuple):\n                agg_name = col[1].__name__ if callable(col[1]) else col[1]\n                new_cols.append(f\"{col[0]}_{agg_name}\")\n            else:\n                new_cols.append(col)\n        grouped_num.columns = new_cols\n        grouped_num = grouped_num.reset_index()\n\n        # Agrega colunas categóricas (se existirem) e faz merge com agregados numéricos\n        if len(cat_aggs) > 0:\n            grouped_cat = df.groupby('id').agg(cat_aggs).reset_index()\n            grouped = grouped_num.merge(grouped_cat, on='id')\n        else:\n            grouped = grouped_num\n\n        return grouped\n\n    # Agrega o conjunto de teste por paciente\n    df_test_agg = aggregate(df_test)\n\n    # Remove coluna 'id' para obter matriz de features compatível com o modelo\n    X_test = df_test_agg.drop(columns=['id'])\n\n    # Faz previsões por paciente\n    preds = model.predict(X_test)\n\n    # Cria DataFrame de submissão e persiste em CSV\n    sub = pd.DataFrame({'id': df_test_agg['id'].values, 'diagnostico': preds})\n    sub.to_csv(output, index=False)\n    print(f'Wrote submission to {output}')",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 8
    },
    {
      "id": "d4133c70-8c21-4910-9678-bfcc66c44566",
      "cell_type": "markdown",
      "source": "Para configurar os argumentos da função main responsável pelas previsões, permitindo manipular o modelo a ser usado e o nome do ficheiro de submissão a ser criado, é usado auxiliarmente o seguinte trecho de código:",
      "metadata": {}
    },
    {
      "id": "8f75f7f7-dcb5-4c72-99e3-e8d0d832f1c5",
      "cell_type": "code",
      "source": "if __name__ == '__main__':\n    parser = argparse.ArgumentParser()\n    parser.add_argument('--model', type=str, default='models/model.pkl')\n    parser.add_argument('--test', type=str, default='data/teste.csv')\n    parser.add_argument('--output', type=str, default='outputs/submission.csv')\n    args = parser.parse_args()\n    # A linha em baixo está em comentário para não executar a função main e evitar erro na célula\n    # main(model_path=args.model, test_csv=args.test, output=args.output)",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 9
    },
    {
      "id": "b31d86cc-a28e-457c-82fe-c86d27a4c2ec",
      "cell_type": "markdown",
      "source": "## Estratégia para Pesquisa e Escolha dos Modelos\nA nossa estratégia para a pesquisa dos modelos foi inicialmente testar todos os diferentes algoritmos aprendidos no decorrer das aulas, e juntar com ensemble os que tiveram melhor score na parte pública do conjunto de teste. \n\nPara a construção dos modelos utilizámos sempre GridSearchCV para procurar os melhores hiperparâmetros e optámos sempre por usar validação cruzada estratificada com 5 folds pois decidimos que era a melhor configuração para a validação cruzada, após fazer alguns testes. Além disso utilizámos sempre os mesmos atributos e transformações mencionados anteriormente na agregação por fatias para eliminar a necessidade de tratar múltiplas linhas por paciente na fase de treino/avaliação.\n\nA escolha dos 3 modelos finais resumiu-se principalmente à nossa percepção da capacidade de generalização dos modelos, de forma a evitar sobre-ajustamento ao conjunto de treino e assegurar a capacidade de generalização em dados novos, visando maximizar o score na parte privada do conjunto de teste.",
      "metadata": {}
    },
    {
      "id": "44e67067-af8c-41a4-a09a-4a20aa1f746e",
      "cell_type": "markdown",
      "source": "## Descrição do Subconjunto dos Modelos Criados\nTodos os modelos criados e submetidos envolveram o uso de algoritmos como: LogisticRegression, K-Nearest Neighbors, Support Vector Machine, Naive Bayes, Decision Tree, Random Forest, Extra Trees, Gradient Boosting Trees e Multi-Layer Perceptron. \n\nEm acréscimo, em todos os modelos submetidos foi aplicado o GridSearchCV para a procura dos melhores hiperparâmetros, assim como validação cruzada estratificada com 5 folds, com exceção de um modelo em que usamos 10 folds em vez de 5. Para a procura dos melhores hiperparâmetros, nós testamos para cada algoritmo os hiperparâmetros mais usuais e indicados nas aulas teóricas.\n\nNós fizemos questão de mencionar o algoritmo usado em cada modelo submetido no kaggle, incluindo o nome do modelo no nome do ficheiro de submissão, apesar de que, nas nossas primeiras submissões, não o fizemos, pois ainda não estávamos totalmente organizados. ",
      "metadata": {}
    },
    {
      "id": "e2ce370c-ed03-4c4f-bc1e-82b382b7f0d1",
      "cell_type": "markdown",
      "source": "## Caracterização dos 3 Modelos Submetidos\n### submission_lr_0.8166.csv\nNeste modelo foi usado o algoritmo LogisticRegression e validação cruzada estratificada com 5 folds. Para o GridSearchCV testámos os seguintes hiperparâmetros:",
      "metadata": {}
    },
    {
      "id": "731cdc19-55c9-41b3-b017-5ffd13a385d9",
      "cell_type": "code",
      "source": "param_grid = {\n        'model__C': [0.001, 0.01, 0.1, 1, 10, 100],\n        'model__solver': ['lbfgs', 'liblinear', 'saga'],\n        'model__penalty': ['l2', 'l1'],\n        'model__class_weight': [None, 'balanced']\n    }",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 10
    },
    {
      "id": "61276139-e4a7-4ab8-8f69-44e0a475411f",
      "cell_type": "markdown",
      "source": "Sendo que os melhores hiperparâmetros encontrados foram:",
      "metadata": {}
    },
    {
      "id": "fc04dafc-7517-465c-81be-19a97f07ebd7",
      "cell_type": "code",
      "source": "best_param = {\n  \"model__C\": 0.01,\n  \"model__class_weight\": \"balanced\",\n  \"model__penalty\": \"l2\",\n  \"model__solver\": \"liblinear\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 11
    },
    {
      "id": "2cc8a004-a14e-46c2-9cd8-7af5a0837293",
      "cell_type": "markdown",
      "source": "Além disso, como indicado no nome do ficheiro, o F1-Weighted score no conjunto de treino completo foi 0.8166.\n\nO desempenho deste modelo na parte pública do conjunto de teste foi de 0.769, enquanto que na parte privada foi 0.709.",
      "metadata": {}
    },
    {
      "id": "a3b67146-2f84-49f8-a0a8-944d26b0cbd6",
      "cell_type": "markdown",
      "source": "### submission_svm_0.7852.csv\nNeste modelo foi usado o algoritmo Support Vector Machine e validação cruzada estratificada com 5 folds. Para o GridSearchCV testámos os seguintes hiperparâmetros:",
      "metadata": {}
    },
    {
      "id": "7edaaf38-6774-4af1-b2b1-1dc9f9e43957",
      "cell_type": "code",
      "source": "param_grid = {\n            'model__C': [0.01, 0.1, 1, 10, 100],\n            'model__kernel': ['linear', 'rbf', 'poly'],\n            'model__gamma': [0.001, 0.01],\n            'model__degree': [2, 3],\n            'model__class_weight': [None, 'balanced']\n        }",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 12
    },
    {
      "id": "5b43b27c-4bf5-4fab-aafa-0ab123847e33",
      "cell_type": "markdown",
      "source": "Sendo que os melhores hiperparâmetros encontrados foram:",
      "metadata": {}
    },
    {
      "id": "b6997aac-a775-4086-b1dc-af753f53e4d2",
      "cell_type": "code",
      "source": "best_param = {\n  \"model__C\": 1,\n  \"model__class_weight\": None,\n  \"model__degree\": 2,\n  \"model__gamma\": 0.001,\n  \"model__kernel\": \"rbf\"\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 13
    },
    {
      "id": "e072f262-0a7f-40ef-98e7-2f493e288643",
      "cell_type": "markdown",
      "source": "Além disso, como indicado no nome do ficheiro, o F1-Weighted score no conjunto de treino completo foi 0.7852.\n\nO desempenho deste modelo na parte pública do conjunto de teste foi de 0.740, enquanto que na parte privada foi 0.600.",
      "metadata": {}
    },
    {
      "id": "3fda3597-912c-4ea5-9c5b-0213b8b7612a",
      "cell_type": "markdown",
      "source": "### submission_nb_0.7500.csv\nNeste modelo foi usado o algoritmo Naive Bayes e validação cruzada estratificada com 5 folds. Para o GridSearchCV testámos os seguintes hiperparâmetros:",
      "metadata": {}
    },
    {
      "id": "0ca4f71a-a257-402f-a264-a568daf65002",
      "cell_type": "code",
      "source": "param_grid = {\n            'model__var_smoothing': [1e-12, 1e-11, 1e-10, 1e-9, 1e-8, 1e-7]\n        }",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 14
    },
    {
      "id": "be2201df-a645-4f39-ac24-23312113c18d",
      "cell_type": "markdown",
      "source": "Sendo que os melhores hiperparâmetros encontrados foram:",
      "metadata": {}
    },
    {
      "id": "5e9df325-d7df-4b64-bf2c-dcf7ce988354",
      "cell_type": "code",
      "source": "best_param = {\n  \"model__var_smoothing\": 1e-12\n}",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 15
    },
    {
      "id": "79133ff2-3ae3-485a-89b1-e8ff4541ffef",
      "cell_type": "markdown",
      "source": "Além disso, como indicado no nome do ficheiro, o F1-Weighted score no conjunto de treino completo foi 0.7500.\n\nO desempenho deste modelo na parte pública do conjunto de teste foi de 0.740, enquanto que na parte privada foi 0.705.",
      "metadata": {}
    },
    {
      "id": "d7e1ba35-01fd-49a9-b0ef-9236e419c357",
      "cell_type": "markdown",
      "source": "### Observações\nÉ ainda importante mencionar que nos 3 diferentes modelos escolhidos foi usada a mesma engenharia de atributos, recorrendo à função inicialmente apresentada responsável pela agregação das fatias, em que para cada coluna numérica calcula-se mean, std, min, max, median, skew e kurt (kurtose), adicionando ainda 'n_slices' como contagem de fatias por paciente.",
      "metadata": {}
    },
    {
      "id": "eaf64721-2f61-477b-b74b-b52cd8f5481a",
      "cell_type": "markdown",
      "source": "## Discussão e Conclusões\nAo usar os diferentes algoritmos mencionados e tendo como referência o desempenho dos modelos na parte pública, chegámos à conclusão que comités de árvores de decisão (Random Forest, Extra Trees e Gradient Boosting Machine) não generalizavam tão bem em dados nunca vistos como algoritmos mais simples, como por exemplo Naive Bayes e LogisticRegression. Nós achamos que a justificação para isto deve-se ao tamanho do conjunto de dados, que é pequeno/médio, e também à sensibilidade do modelo a pequenas variações nos dados de treino e, por isso, algoritmos como Random Forest vão sobre-ajustar-se facilmente ao conjunto de treino e, consequentemente, não terem um bom desempenho em dados novos.\n\nDito isto, na escolha final dos 3 modelos, nós escolhemos modelos construídos sem comités de árvores de decisão, visando evitar complexidade e maximizando a capacidade de generalização em dados novos.\n\nQuanto aos modelos finais escolhidos, o desempenho dos mesmos teve pequenas diferenças da parte pública para a privada, à exceção do modelo em que foi usado o algoritmo SVM, que teve uma diferença de desempenho de 0.140, sendo o menor score o da parte privada, o que demonstra má generalização por parte do modelo.\n\nAcrescentando, o modelo submetido no Kaggle pela nossa equipa com o desempenho mais alto na parte privada do conjunto de teste foi o submission_dt_0.8314.csv, o qual foi construído com o algoritmo Decision Tree. O seu desempenho na parte pública foi de 0.580, enquanto que na privada foi de 0.833. Achámos este resultado estranho inicialmente, mas o mesmo pode ser explicado pela eventual e possível existência de variância nas amostras da parte pública para a parte privada e, consequentemente, o modelo pode \"ter azar\" na parte pública, mas performar melhor na parte privada.",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true
      }
    }
  ]
}